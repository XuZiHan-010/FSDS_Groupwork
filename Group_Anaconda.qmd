---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Anaconda's Group Project
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import os
from requests import get
from urllib.parse import urlparse

def cache_data(src:str, dest:str) -> str:
    """
    Returns Geopandas Data Frame from dest path.
    If file does not exist in local, retrieves files from source URL and makes directory.

    Attributes
    ----------

    src : str
        Source URL of file

    dest : str
        Path to save fetched file
        
    """    
    url = urlparse(src) # We assume that this is some kind of valid URL 
    fn  = os.path.split(url.path)[-1] # Extract the filename
    dfn = os.path.join(dest,fn) # Destination filename
    
    if not os.path.isfile(dfn):
        
        #print(f"{dfn} not found, downloading!")

        path = os.path.split(dest)
        
        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)
            
        with open(dfn, "wb") as file:
            response = get(src)
            file.write(response.content)
            
        #print("\tDone downloading...")

    else:
        #print(f"Found {dfn} locally!")
        pass

    # return dfn


cache_data('https://raw.githubusercontent.com/XuZiHan-010/FSDS_Groupwork/main/harvard-cite-them-right.csl', '.')
cache_data('https://raw.githubusercontent.com/XuZiHan-010/FSDS_Groupwork/main/bio.bib','.')
```

## Declaration of Authorship {.unnumbered .unlisted}

We, [Anaconda], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:19/12/2023

Student Numbers: 23109051, 23035207, 23049578, 23075632, 23140058

## Brief Group Reflection

| What Went Well                     | What Was Challenging |
| --------------                     | -------------------- |
| Rational division of tasks         | Effective Communication |
| Comprehensive resource collection  | Detailed project schedule |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?
We really want to know what kind of maps can be added to Question 7 that will be more beneficial to our research (such as the types of pictures and independent and dependent variables) according to our research questions?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

## 1. Who collected the data? 

The data is collected by Inside Airbnb (IA) from AirBnB’s official website. IA was launched in 2016 by Murray Cox  [@insideairbnb], an independent researcher and activist who started data scraping in 2014 [@Katz2017]. 


## 2. Why did they collect it? 

IA started the project to empower local communities to understand the impacts of Airbnb on housing market [@insideairbnb]. It found that Airbnb misled media and public by purging the over 1000  “Entire Home” listings in the New York City before releasing data in 2015. The purging resulted in misrepresentation of actual “Entire Home” listings and multiple listings by hosts. [@Cox2016] Hosts buy and list residential properties (not their primary homes) on Airbnb resulting into more pressure on the residential market , reducing availability, increasing prices and rentals, and disrupting local communities. The long-term median rentals in New York city have increased by $380 between 2014 and 2017 due to reduction in availability of housing caused by Airbnb. [@Casas2018] 


## 3. How was the data collected? 

IA collects data by web scraping from the Airbnb website, using only publicly available information such as listings, availability calendar for 365 days in future and reviews for each listing. The data for each region is collected periodically and made available on quarterly basis [@insideairbnb]. The data is collected using python scripts available online. [@Alsudais2021] One such resource is Github repository of Slee. The scripts available in the repository are not updated anymore [@Slee2023]. It is unclear whether IA update the scripts, raising concerns regarding the performance of scraping process used by IA. The data collection also suffer due to constant changes on the website by Airbnb to discourage scrapers.[@dirkmjk2019]

On top vague data collection process, the actual data is also inconsistent [@Adamiak2019]. The data quality issues arise from two key aspects, (1) the data only shows a snapshot of listings and calendar availability at the time when data was scraped, and (2) it is impossible to understand whether the listing is unavailable because it is booked by a guest or blocked by the host. [@Leonel2022]


## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise? 

Airbnb is not keen to let scraper extract data and analyze its impact on housing markets. Airbnb make changes to their website that breaks the scrapers code. Amsterdam’s digital surveillance department gave up scraping Airbnb since it became difficult to update their scraper code [@dirkmjk2019]. Website changes, access controls can introduce gaps in raw data extraction affecting the accuracy, completeness and consistency of data.
The data collected through scraping can lack contextual information and have algorithm bias [@Boegershausen2022] and can be manipulated. 
IA takes a snapshot of publicly available information on Airbnb website which only cover listings available at the time of scraping. Airbnb can easily manipulate this by deleting listings, allowing them to control the narration of the dataset. [@Williams2020]. The removal of over 1000 “Entire Home” listing in New York City is a prime example of this manipulation. [@Cox2016]
The Airbnb calendar data (for 365 days in future) for a listing does not differentiate between a booked night vs an unavailable night. [@insideairbnb] Booked nights are indicative of availability of property for renting over the year strengthening IA’s argument of impact on housing market. On contrary, unavailable property (due to blocked by host) weakens the argument. The IA algorithm assumes all booked and unavailable listings as “unavailable”, which may lead to underrepresentation of impacts of Airbnb on housing market.


## 5. What ethical considerations does the use of this data raise? 

There are few ethical questions that must be considered while using IA data-

IA collects data such as names, photographs, listings and reviews that are publicly available on Airbnb [@insideairbnb] however some of this data may be classified as personal data per GDPR rules, that may relate to an identified or identifiable individual [@ICO2023]. While Airbnb may have taken consent for use of personal data, the data available through IA may breech the requirement of obtaining prior consent from individual [@ICO2023]. Further, the scraping process used for gathering data does not guarantee the that no sensitive information is collected by accident or otherwise, which if mishandled can lead to criminal penalties and fines [@Intersoft2018]. Accessing such data through IA raises serious ethical concerns. Moreover, the data prone to transferred to other parts of the worlds without explicit consent, in violation of privacy policy [@Airbnb2021].  Additionally, by scraping data from Airbnb, IA violates the standard terms of services of Airbnb platform that may have legal repercussion. [@Airbnb]

IA’s research points to significant impact in many cities with reduction of housing [@insideairbnb], potentially overlooking other critical contributing factors and presenting a biased opinion. IA data quality and its reliability must be considered with additional information on local communities, residential patterns and regulations to generate an unbiased conclusion of impact on communities. 


```{python}
# import packages
import os
import warnings
import bibtexparser
import pandas as pd
import numpy as np
import seaborn as sn
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from matplotlib.gridspec import GridSpec
```

```{python}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '2023-09-06-listings.csv.gz'
url  = f'{host}/{path}/{file}'

if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
  df.to_csv(file)
```

```{python}
# load the datasets
airbnb_data = pd.read_csv('http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz', low_memory= False )
# load the rental market dataset
housing_data = pd.read_csv('https://github.com/XuZiHan-010/FSDS_Groupwork/raw/main/housing_data.csv',skiprows=2)
# load the geo-spatial data
london_geodata = gpd.read_file('http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/visualisations/neighbourhoods.geojson')
```

```{python}
# !wget https://github.com/XuZiHan-010/FSDS_Groupwork/blob/main/bio(5).bib
```

```{python}
# Data wrangled and filtered due to specific reasons, discussed in Question 7.
# Filter out the columns
columns_to_keep = ['host_total_listings_count', 'host_since', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'bedrooms', 'beds', 'price', 'availability_365']
airbnb_data = airbnb_data[columns_to_keep]

# Filter the data to include only listings that are available
airbnb_data = airbnb_data[airbnb_data['availability_365'] > 0]

#select the numerical columns
numerical_columns = ['host_total_listings_count','price', 'host_since', 'bedrooms', 'beds','latitude','longitude']

# Calculate descriptive statistics for categorical columns
categorical_columns = ['property_type', 'neighbourhood_cleansed']

# as we have price saved as a format like $130,000.00 so we need to deal with the '$' and ',' to convert it to numeric
airbnb_data['price'] = airbnb_data['price'].astype(str)
airbnb_data['price'] = airbnb_data['price'].str.replace('$','', regex=False).str.replace(',','').astype('float')
#convert host_since to the date format 
airbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])

#define the property types which we won't use 
exclude_types = ['Boat', 'Camper/RV', 'Houseboat', 'Religious building', 'Room in aparthotel']
#select the property type
airbnb_data = airbnb_data[~airbnb_data['property_type'].isin(exclude_types)]

# Define the start and end date for the period
start_date = pd.to_datetime('2016-01-01')
end_date = pd.to_datetime('2018-12-31')

# Filter the DataFrame for rows where the host_since falls within the specified range(2016-2018)
airbnb_data = airbnb_data[(airbnb_data['host_since'] >= start_date) & (airbnb_data['host_since'] <= end_date)]

#only need the airbnb_data which host_listings is less than two
airbnb_data = airbnb_data[airbnb_data['host_total_listings_count']<2]


# #check whether the columns in numerical_columns is numerical or not
# for column in numerical_columns:
#     if column in airbnb_data.columns:
#         #print(f"Data type of '{column}': {airbnb_data[column].dtype}")
#         if pd.api.types.is_numeric_dtype(airbnb_data[column]):
#             #print(f"{column} is numeric.")
#         else:
#             #print(f"{column} is not numeric.")
#     else:
#         #print(f"{column} is not in the DataFrame.")
```

```{python}
# #view the shape of data frame
# print(airbnb_data.shape)
```

```{python}
#check na for london geo-spatial data
na_count = london_geodata.isna().sum()
# print(na_count)
```

```{python}
#drop the empty column
london_geodata = london_geodata.dropna(axis=1)
```

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

By wrangling and filtering out the Airbnb listings that are out of our research scope, we have found that there are 3756 observations in this dataset. We have conducted descriptive statistics analyses based on our Airbnb dataset using bar chart, boxplot and scatterplot. Our main findings are as below:

(1) Price: The average nightly price of £161.58 masks a vast range, with listings spanning from £8 to £20,362. This variance reflects the diverse offerings within Airbnb, catering to varied budgets and styles according to renters’ diverse preferences in London. The standard deviation of £421.86 further highlights the significant spread, hinting at the potential influence of factors of location, amenities, and seasonality.

```{python}
# Descriptive statistics for the dataset
# write the descriptive statistics into a txt file which seems to be easy for analysis
# for column in numerical_columns:
#     # print(f"Descriptive statistics for {column}:")
#     # print(str(airbnb_data[column].describe()))
     
# for column in categorical_columns:
#     # print(f"Unique values for {column}:\n")
#     # print(str(airbnb_data[column].value_counts()))
```

```{python}
#actually we don't have na in our housing dataset but we have symbol . instead which fill in the empty cell
#set a boolean mask to remove these dots
mask_dot = (housing_data == '.').any(axis=1)
mask_comma = (housing_data == ',').any(axis=1)
mask_double_dot = (housing_data == '..').any(axis=1)

# Combine the masks
combined_mask = mask_dot | mask_comma | mask_double_dot

# Filter the DataFrame
housing_data = housing_data[~combined_mask]
# Filter out the columns
condition = (
    (housing_data['Year'] >= 2016) &
    (housing_data['Year'] <= 2018))

housing_data = housing_data[condition]

# #Descriptive statistics for 'Count of rents
# print("Descriptive statistics for 'Count of rents':")
# print(str(housing_data['Count of rents'].describe()))

# #Descriptive statistics for 'Average
# print("Descriptive statistics for 'Average':")
# print(str(housing_data['Average'].describe()))
    
# # Value counts for 'Area'
# print("Value counts for 'Area':")
# print(str(housing_data['Area'].value_counts()))
   

# # Value counts for 'Category'
# print("Value counts for 'Category':")
# print(str(housing_data['Category'].value_counts()))
```

(2) Number of Bedrooms: Our result indicates that among all types of Airbnb listings from 2016 to 2018, the majority of Airbnb rooms only have 1 bedroom. Rooms with 6 bedrooms would cost the highest (700 pounds per day on average), while rooms with 1 bedroom only would cost the lowest (nearly 100 pounds per day on average). It also suggests that Airbnbs with 4 bedrooms has the highest variation in price, ranged from 300 to 1000 pounds per day. 

```{python}
# make catplot of airbnb average daily price~bedrooms
plt.figure(figsize=(12, 10))
sn.catplot(x='bedrooms', y='price', data=airbnb_data, kind='bar', color='skyblue')
# The black vertical line on some bars represents the variability and uncertainty in the data
# The longer the line, the more dispersed the data is
plt.xlabel("Number of Airbnb's Bedrooms")
plt.ylabel('Airbnb Average Daily Price (Unit: Pounds)')
plt.title('Catplot of Airbnb Average Daily Price~Bedrooms')
plt.show()
warnings.filterwarnings('ignore')
```

(3) Property Preference: There are 46 unique types of property listed according to our dataset, which provides short-term renters a comprehensive choice of listings. With 1,143 entire rental units, a clear preference for renting out full apartments or houses dominates the market. This choice may align with travellers’ desire for autonomy and privacy during their stay, while one the other hand, it may resonate with hosts seeking greater potential income compared to shared spaces.

```{python}
# make boxplot of airbnb daily price~property type
plt.figure(figsize=(50, 8))
sn.boxplot(x='property_type',
            y='price',
            data=airbnb_data[airbnb_data.price <= 1000],
            color='orange'
            )
plt.ylabel('Airbnb Daily Price (Unit: Pounds)')
plt.xlabel('Airbnb Property Type')
plt.title("Boxplot of Airbnb Daily Price~property Type", y=1.02)
plt.xticks(rotation=45)
plt.show() # Double click the graph to zoom in if it is too small
warnings.filterwarnings('ignore')
```

(4) Boroughs: The spatial distribution of Airbnb listings reveals a distinct concentration in Lambeth, which boasts the highest number (275) across the three years. This contrasts sharply with the City of London's mere 6 listings. This disparity likely stems from a combination of factors. Lambeth's larger landmass provides more housing options suitable for short-term rentals, while its vicinity to popular tourist destinations like Westminster Abbey and Buckingham Palace further fuels demand.

```{python}
# make scatterplot of airbnb dispersion in London
plt.figure(figsize=(12,8))
sn.scatterplot(data=airbnb_data, x='longitude', y='latitude', hue='neighbourhood_cleansed')
plt.ylabel('Latitude of Airbnb')
plt.xlabel('Longitude of Aribnb')
plt.title('Scatterplot of Airbnb Dispersion in London', y=1.02)
# Place the legend outside the plot
legend = plt.legend(title='London Boroughs', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
plt.show()
warnings.filterwarnings('ignore')
```

From individual hosts sharing their homes to professional management companies catering to larger groups, the platform caters to a wide range of budgets and preferences. The price variations tied to bedrooms further highlight the variety of experiences available, from budget-friendly solo adventures to luxurious group getaway Our data paints a picture of a vibrant and multifaceted Airbnb landscape in London.


## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

The dataset we have used encompassing London's Airbnb listings and long-term rental data, would enable us to investigate the relationships between single-listing Airbnb hosts and long-term rentals. Now, we can leverage it to tackle the crucial question: Could a heavier sales tax on single-listing Airbnb hosts influence the long-term housing market?

### Quantifying the Localized Impact:
To address this, we can delve deeper into our data, employing tools such as scatter plots and heatmaps to visualize the relationships between single-listing STLs and long-term housing availability and prices. This granular analysis, focusing on specific neighborhoods and boroughs, will reveal which areas are most affected and which housing types face the brunt of the impact. For example, comparing Lambeth's concentration of STLs with its long-term rental scarcity could expose a crucial correlation.

### Predicting Policy Scenarios:
But simply observing these interactions isn't enough. We can utilize statistical models, specifically a difference-in-differences (DID) approach, to simulate the potential outcomes of different policy scenarios. Imagine testing the proposed sales tax, comparing housing market changes before and after its implementation in specific areas. This allows us to assess the predicted impact on both long-term housing availability and prices, providing policymakers with valuable insights before enacting real-world changes.

### Addressing Nuances and Limitations:
However, we shall acknowledge data limitations and the contextual differences between London and Taiwan. Biases within the data and contextual differences necessitate caution in extrapolating our findings. While Chen et al.'s study in Taiwan offers valuable context, London's unique dynamics require careful consideration. In this study, we have specifically taken into consideration of the following influential factors:
(1) Temporal scope: We restricted our analysis to the period January 2016 to December 2018, as this timeframe encompasses the most recent available data for both short-term and long-term rental listings in London.
(2) Listing Availability: We limited our analysis to active, available Airbnb listings, ensuring that the data reflects the current state of the short-term rental market.
(3) Host Ownership: We confined our examination to Airbnb hosts who exclusively own one single short-term Airbnb listing in London. Since London’s current sales tax policy only applies to hosts who own two or more short-term Airbnb listings, we shall focus on hosts with one single listing specifically. 
(4)	Housing Type: Other housing types aside from entire house/apt, e.g. hotel rooms or private rooms, cannot be transitioned into long-term listings, consequently having no impact on the quantity and pricing of long-term listings.

However, other variables we may not be able to control in this research using the datasets above, such as amenities, spatial variations and external socio-economic factors between London and Taiwan. Further research may be necessary for providing a more comprehensive insights for policy suggestions. 

With such exploration and uses of the dataset, we may test the effectiveness of whether implementing heavier sales tax on single-listing Airbnb hosts would produce positive effects to London's long-term housing market.

```{python}
# We'll focus on the entire rental unit property type and the year spell 2016-2018 for plotting the maps

# Extract the year from 'host_since' and create a new 'year' column format like 2011 from 2011-01-01
airbnb_data['year'] = airbnb_data['host_since'].dt.year

# Select entire rental unit only as we'll focus on it the df_properities will be a template variable which only has entire rental unit for property type
df_properties = airbnb_data[airbnb_data['property_type'] == 'Entire rental unit']

# Specify the year spell as we are focus on 2016-2018
years = [2016, 2017, 2018]
```

```{python}
# plotting the map average daily price for airbnb entire rental unit for year 2016-2018 to visualize the change of airbnb daily price borough by borough

# Group and calculate average price by region and year
grouped_data = df_properties.groupby(['neighbourhood_cleansed', 'year'])['price'].mean().reset_index()

# Pivot the data to have years as columns and regions as rows
pivoted_data = grouped_data.pivot(index='neighbourhood_cleansed', columns='year', values='price')
legend_handle = [Patch(facecolor='lightgrey', edgecolor='lightgrey', label='No Entire Rental Unit in this borough')]
# Merge the GeoDataFrame with the pivoted data only once
gdf = london_geodata.merge(pivoted_data, how='left', left_on='neighbourhood', right_index=True)

# fig, axs = plt.subplots(len(years), 1, figsize=(14, 10 * len(years)))  
fig = plt.figure(figsize=(14, 30))
gs = GridSpec(3, 2, width_ratios=[2.5, 1.5])

for i, year in enumerate(years):
    # Create the choropleth map subplot for each year on the left
    ax_map = fig.add_subplot(gs[i, 0])
    # Plot the GeoDataFrame
    gdf.plot(column=year, ax=ax_map, cmap='viridis', legend=True, missing_kwds={'color': 'lightgrey'},
             legend_kwds={'label': "Airbnb Average Daily Price", 'orientation': "horizontal"})
    ax_map.set_title(f'Map of Average price for Entire rental unit in {year}')
    ax_map.axis('off')

    # Remove the axes spines (the black frame)
    for spine in ax_map.spines.values():
        spine.set_visible(False)

    # Adjust the legend
    ax_map.legend(handles=legend_handle, loc='lower center', bbox_to_anchor=(0.5, -0.05), fontsize='large')

    # Create the heatmap subplot for each year on the right
    ax_heatmap = fig.add_subplot(gs[i, 1])
    # Filter and create the heatmap data
    df_year = df_properties[df_properties['year'] == year]
    avg_prices = df_year.groupby('neighbourhood_cleansed')['price'].mean().sort_values(ascending=False)
    heatmap_data = avg_prices.to_frame(name='Average Price')

    # Plot the heatmap
    sn.heatmap(heatmap_data, ax=ax_heatmap, annot=True, fmt='.0f', annot_kws={"size": 6}, cmap='viridis')
    ax_heatmap.set_title(f'Heatmap of Average price for Entire rental unit in {year}')
    ax_heatmap.set_ylabel('Boroughs')
    ax_heatmap.set_yticklabels(avg_prices.index, rotation=0)


plt.tight_layout()

plt.show()
```

```{python}
#convert the average variable of housing data to float type
housing_data['Average'] = housing_data['Average'].astype(str)
housing_data['Average'] = housing_data['Average'].str.replace(',','').astype('float')
housing_data['Count of rents'] = housing_data['Count of rents'].astype(str)
housing_data['Count of rents'] = housing_data['Count of rents'].str.replace(',', '').astype('float')
```

```{python}
#Plotting the hosuing average borough by borough in year 2017 and year 2018
#group by the area and year 
#actually the Average here already stands for the Average price but just in case we have more than one average per year as if there is only one value mean will still be itself
grouped_data = housing_data.groupby(['Area', 'Year'])['Average'].mean().reset_index()

# Pivot the data to have years as columns and areas as rows
pivoted_data = grouped_data.pivot(index='Area', columns='Year', values='Average')

years = [2017, 2018]

fig, axs = plt.subplots(2, 1, figsize=(14, 10 * len(years))) 
legend_handle = [Patch(facecolor='lightgrey', edgecolor='lightgrey', label='No value in this borough')]
for i, year in enumerate(years):
    # Plot the choropleth map for each year
    ax = axs[i]
    gdf.plot(column=year, ax=ax, legend=True,
             legend_kwds={'label': "Average Price of Hosuing data", 'orientation': "vertical"},
             cmap='Oranges',  
             missing_kwds={'color': 'lightgrey'})  
    ax.set_title(f'Average Price of Housing Data in {year}')
    ax.axis('off') 
    # add the legend for demonstrating the gray patch representing no value in that area
    axs[i].legend(handles=legend_handle, loc='lower right', fontsize='large')

plt.tight_layout(pad=3.0)
# plt.savefig('airbnb_listings_count_map.png', dpi=600) 
plt.show()
```

## References
